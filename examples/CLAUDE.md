# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This directory contains self-contained examples for the NVIDIA NeMo Agent Toolkit (NAT). Each example demonstrates specific NAT features and integration patterns using Python 3.11+ and managed with `uv`.

Current examples:
- **`mcp_rag_demo/`** - Demonstrates MCP (Model Context Protocol) server with agentic RAG for support ticket search using Milvus vector database
- **`asset_lifecycle_management/`** - Complex multi-agent system for industrial asset management with predictive maintenance, anomaly detection, and SQL generation

## Building and Testing

### Initial Setup

From the repository root:

```bash
# Fetch LFS files (required for datasets)
git lfs install
git lfs fetch
git lfs pull

# Create virtual environment
uv venv --python 3.13 --seed .venv
source .venv/bin/activate

# Install development dependencies
uv sync --dev
```

### Installing Individual Examples

**IMPORTANT: Always use `uv` for package management, NOT `pip`**

```bash
# Install an example (from repository root)
uv pip install -e examples/mcp_rag_demo
uv pip install -e examples/asset_lifecycle_management

# Or from within the example directory
cd examples/asset_lifecycle_management
uv pip install -e .

# Install with optional dependencies (ALM example)
uv pip install -e "examples/asset_lifecycle_management[postgres,mysql]"

# Install with E2B cloud sandbox support (no Docker needed)
uv pip install -e "examples/asset_lifecycle_management[e2b]"

# Install all optional dependencies
uv pip install -e "examples/asset_lifecycle_management[all]"
```

### Running Tests

```bash
# Run all tests
pytest

# Run tests for a specific example
pytest examples/mcp_rag_demo/tests/
pytest examples/asset_lifecycle_management/

# Run specific test
pytest examples/asset_lifecycle_management/test_alm_workflow.py -k "test_name" -v

# Run with markers (ALM example uses e2e marker)
pytest examples/asset_lifecycle_management/test_alm_workflow.py -m e2e -v
```

### Linting and Code Style

```bash
# Run all checks from repository root
./ci/scripts/checks.sh

# Run only Python checks
./ci/scripts/python_checks.sh

# Run pre-commit hooks manually
pre-commit run --all-files --show-diff-on-failure
```

Code style enforced:
- **ruff** for linting and import sorting (max line length: 120)
- **yapf** for code formatting
- **pylint** for code quality
- **vale** for documentation
- All files must include SPDX copyright header

## NAT Component Architecture

### Registration Pattern

All NAT components use a plugin-based registration system:

1. **Config class** inherits from `FunctionBaseConfig` with a `name` parameter
2. **Function** decorated with `@register_function(config_type=...)`
3. **Entry point** in `pyproject.toml` under `[project.entry-points.'nat.components']`
4. **register.py** imports all tools to trigger registration

Example structure:
```python
# Define config
class MyToolConfig(FunctionBaseConfig, name="my_tool"):
    param: str = Field(description="Parameter")

# Register function
@register_function(config_type=MyToolConfig)
async def my_tool_function(config: MyToolConfig, builder: Builder):
    class InputSchema(BaseModel):
        query: str = Field(description="Query")

    async def _execute(query: str) -> str:
        # Access config.param
        # Get LLMs/embedders: await builder.get_llm(name)
        return result

    yield FunctionInfo.from_fn(
        _execute,
        input_schema=InputSchema,
        description="What this tool does"
    )
```

### YAML Configuration

NAT workflows are declarative YAML files with four main sections:

```yaml
llms:                          # LLM definitions
  my_llm:
    _type: nvidia_chat_model   # or: nim, openai, etc.
    model_name: "meta/llama-3.1-70b-instruct"
    api_key: "${NVIDIA_API_KEY}"

embedders:                     # Embedder definitions
  my_embedder:
    _type: nvidia_embeddings
    model_name: "nvidia/nv-embedqa-e5-v5"

functions:                     # Tool configurations
  my_tool:
    _type: my_tool             # Must match FunctionBaseConfig name
    param: "value"
    llm_name: "my_llm"         # Reference by name

workflow:                      # Agent orchestration
  _type: react_agent           # or: other agent types
  tool_names: [my_tool]
  llm_name: my_llm
  max_iterations: 10
```

Components reference each other by name strings, resolved at runtime by NAT's builder.

### Directory Structure

Standard NAT example structure (generated by `nat workflow create`):

```
examples/$EXAMPLE_NAME/
├── configs/                   # Symlink to src/nat_$EXAMPLE_NAME/configs/
├── data/                      # [Optional] Symlink to src/nat_$EXAMPLE_NAME/data/
├── scripts/                   # [Optional] Setup scripts
├── src/
│   └── nat_$EXAMPLE_NAME/     # Module name must start with nat_
│       ├── configs/
│       │   └── config.yml
│       ├── __init__.py
│       └── register.py        # Import all tools here
├── tests/                     # pytest tests
├── README.md
└── pyproject.toml             # Entry points declaration
```

## Running NAT Workflows

```bash
# Run a workflow with input
nat run --config_file examples/mcp_rag_demo/configs/support-ui.yml --input "your query"

# Start an MCP server (exposes tools via MCP protocol)
nat mcp serve --config_file examples/mcp_rag_demo/configs/support-ui.yml --port 9904

# Start a UI server (REST API for frontend)
nat serve --config_file examples/asset_lifecycle_management/configs/config-reasoning.yaml

# Run evaluation
nat eval --config_file examples/asset_lifecycle_management/configs/config-reasoning.yaml
```

## Example-Specific Notes

### MCP RAG Demo

Architecture: 3-terminal setup
1. NAT MCP Server (exposes tools via MCP)
2. NAT UI Server (MCP client providing REST API)
3. Frontend UI (browser interface)

Key tools:
- `search_support_tickets`: Semantic search with NIM embeddings
- `query_by_category`: Filter by category (uses whitelist validation)
- `query_by_priority`: Filter by priority (uses whitelist validation)
- `rerank_support_tickets`: Rerank with NIM reranker

Requires: Milvus vector database running on localhost:19530

### Asset Lifecycle Management

Multi-agent system with specialized tools:
- **SQL Retriever**: Vanna-based SQL generation (TWO implementations - see SQL section below)
- **RUL Prediction**: XGBoost-based remaining useful life prediction
- **Anomaly Detection**: MOMENT time-series model for sensor anomalies
- **Code Execution**: Python code execution (TWO implementations - see below)
- **Code Generation Assistant**: Generates and executes Python code via code execution tools
- **Plotting Agents**: Data visualization tools

#### Dual SQL Tool Implementations

**IMPORTANT**: This example currently has TWO SQL retriever implementations for comparison testing:

1. **Old Implementation (Currently Active)**
   - Location: `src/nat_alm_agent/retrievers/generate_sql_query_and_retrieve_tool.py`
   - Config type: `generate_sql_query_and_retrieve_tool`
   - Config name: `sql_retriever` (active) and `sql_retriever_old`
   - Status: Currently used in workflows
   - Characteristics: 273-line custom implementation with VannaManager wrapper, auto-saves all results to JSON

2. **New Implementation (Package-Based)**
   - Location: External `nat_vanna_tool` package (from NeMo-Agent-Toolkit repo)
   - Config type: `vanna_sql_tool`
   - Config name: `sql_retriever_vanna`
   - Status: Defined in config but not active, auto-registered via entry points
   - Characteristics: Reusable package with `auto_train_on_init: true` flag

**Switching Between Implementations:**

In `configs/config-reasoning.yaml`, change the `sql_retriever` `_type`:

```yaml
# Use old implementation
sql_retriever:
  _type: generate_sql_query_and_retrieve_tool
  # ... rest of config

# Use new implementation
sql_retriever:
  _type: vanna_sql_tool
  vector_store_path: "database_vanna"  # Different path to avoid conflicts
  training_data_path: "vanna_training_data.yaml"  # Note: different param name
  auto_train_on_init: true
  # ... rest of config
```

**Comparison Testing:**

Test suite location: `tests/test_sql_comparison.py`

Run comparison tests:
```bash
pytest tests/test_sql_comparison.py -m comparison -v
```

Test queries cover: count, column selection, aggregation, filtering, RUL retrieval

Comparison metrics: correctness, accuracy, performance, reliability

**Future Plan**: After comparison testing, the losing implementation will be removed. The config comment states: `# Active SQL tool (default to old for now, change after comparison)`

**Documentation in Code**: See `src/nat_alm_agent/register.py:21-24` for inline explanation:
```python
# NOTE: Both SQL implementations are available for comparison testing:
#   1. sql_retriever_old: Original implementation (generate_sql_query_and_retrieve_tool)
#   2. sql_retriever_vanna: New package-based implementation (vanna_sql_tool from nat_vanna_tool)
#      The vanna_sql_tool is auto-registered via entry points in nat_vanna_tool package
from .retrievers import generate_sql_query_and_retrieve_tool
```

This comment pattern is useful for explaining architectural decisions and temporary dual implementations.

#### Dual Code Execution Implementations

**IMPORTANT**: This example has TWO code execution implementations for comparison:

1. **Local Docker Sandbox (Default)**
   - Location: Uses NAT toolkit's built-in `code_execution` tool
   - Type: `code_execution` with `sandbox_type: "local"`
   - Status: Currently used in workflows
   - Characteristics: Requires Docker, mounts local filesystem, direct database access

2. **E2B Cloud Sandbox (New)**
   - Location: `src/nat_alm_agent/code_execution/e2b_code_execution_tool.py`
   - Type: `e2b_code_execution`
   - Status: Available as alternative, tested separately
   - Characteristics: Cloud-hosted, no Docker needed, file upload/download pattern

**Switching Between Implementations:**

In config YAML, change the `code_execution_tool` reference in `code_generation_assistant`:

```yaml
# Use local Docker
code_generation_assistant:
  code_execution_tool: "code_execution_local"

# Use E2B cloud
code_generation_assistant:
  code_execution_tool: "e2b_code_execution"
```

**E2B Configuration:**

```yaml
e2b_code_execution:
  _type: e2b_code_execution
  e2b_api_key: "${E2B_API_KEY}"
  workspace_files_dir: "output_data"
  timeout: 30.0
  max_output_characters: 2000
```

**File Handling Differences:**

- **Local Docker**: Filesystem mounted to `/workspace`, direct file access
- **E2B Cloud**: Files uploaded before execution, downloaded after completion

**Testing E2B:**

Quick test script:
```bash
export E2B_API_KEY="your-key"
python examples/asset_lifecycle_management/test_e2b_sandbox.py
```

Minimal NAT workflow test:
```bash
export E2B_API_KEY="your-key"
export NVIDIA_API_KEY="your-key"
nat run --config_file examples/asset_lifecycle_management/configs/config-e2b-test.yaml \
  --input "Create a JSON file with {'test': 'success'}"
```

See `configs/E2B_TEST_README.md` for detailed testing instructions.

**Comparison Metrics:**
- **Setup**: E2B wins (no Docker) vs Local (requires Docker + container)
- **Speed (cold)**: E2B ~150ms vs Local ~2-5s
- **Speed (execution)**: Local faster (no file transfers) vs E2B (upload/download overhead)
- **Database access**: Local direct vs E2B must upload file (~50-100MB)
- **Cost**: Local free vs E2B API usage
- **Network**: Local not needed vs E2B required
- **Use cases**: E2B for CI/CD, production, cloud environments; Local for development, large databases

**E2B SDK Setup and API Usage:**

The E2B Code Interpreter SDK (v2.x) requires specific initialization:

```python
from e2b_code_interpreter import Sandbox

# Set API key via environment variable (REQUIRED)
export E2B_API_KEY="your-key"

# Create sandbox using Sandbox.create() method
with Sandbox.create() as sandbox:
    execution = sandbox.run_code("print('hello')")
    print(execution.text)
```

**Key Requirements:**
- **Import**: `from e2b_code_interpreter import Sandbox` (NOT `from e2b import Sandbox`)
- **Initialization**: Use `Sandbox.create()` method, NOT `Sandbox(api_key=...)`
- **API Key**: MUST be set via `E2B_API_KEY` environment variable, constructor doesn't accept `api_key` parameter
- **File Operations**: Use `sandbox.files.write()` and `sandbox.files.read()` (NOT `sandbox.filesystem.*`)
- **Default Path**: Files are in `/home/user/` directory (NOT `/workspace/`)

**E2B File Handling Pattern:**

The E2B sandbox automatically manages file transfers:

1. **Before execution** - Uploads to E2B sandbox:
   - `output_data/utils/` → `/home/user/utils/` (Python utilities)
   - `database/nasa_turbo.db` → `/home/user/database/nasa_turbo.db` (if exists)

2. **During execution** - Code runs in isolated E2B environment

3. **After execution** - Downloads from E2B to local:
   - All `.json`, `.html`, `.png`, `.jpg`, `.csv`, `.pdf` files
   - Saved to `output_data/` directory

**Implementation Details:**

E2B sandbox class location: `src/nat_alm_agent/code_execution/e2b_sandbox.py`

Key methods:
- `_setup_workspace()`: Uploads utils and database files to `/home/user/` in sandbox
- `_download_outputs()`: Downloads generated files from sandbox to local `output_data/`
- `execute_code()`: Main execution with error handling

**Implementation Notes (Fixed Jan 2026):**

The E2B sandbox implementation was updated to use the correct E2B SDK v2.x API:

1. **Changed**: `Sandbox(api_key=..., timeout=...)` → `Sandbox.create()`
2. **Changed**: API key now set via environment variable in `__init__`: `os.environ['E2B_API_KEY'] = api_key`
3. **Changed**: File operations updated:
   - `sandbox.filesystem.make_dir()` → removed (not needed with `sandbox.files.write()`)
   - `sandbox.filesystem.write()` → `sandbox.files.write()`
   - `sandbox.filesystem.read()` → `sandbox.files.read()`
   - `sandbox.filesystem.list()` → `sandbox.files.list()`
4. **Changed**: Default paths updated from `/workspace/` to `/home/user/`

The implementation now matches the official E2B documentation at https://e2b.dev/docs

**Testing E2B Implementation:**

```bash
# Direct Python test (4 test cases)
cd examples/asset_lifecycle_management
export E2B_API_KEY="your-key"
python test_e2b_sandbox.py

# Expected output:
# ✅ TEST 1: Simple Code Execution - PASSED
# ✅ TEST 2: File Generation and Download - PASSED
# ✅ TEST 3: Data Processing (Pandas) - PASSED
# ⚠️ TEST 4: Utils Upload - SKIPPED (optional)

# NAT workflow test
export NVIDIA_API_KEY="your-key"
nat run --config_file configs/config-e2b-test.yaml \
  --input "Generate code to create a test JSON file"

# Check downloaded files
ls -la output_data/
```

**E2B Configuration Example:**

```yaml
functions:
  e2b_code_execution:
    _type: e2b_code_execution
    e2b_api_key: "${E2B_API_KEY}"  # From environment variable
    workspace_files_dir: "output_data"  # Local directory for uploads/downloads
    timeout: 30.0  # Longer timeout for file transfers
    max_output_characters: 2000

  code_generation_assistant:
    _type: code_generation_assistant
    llm_name: "coding_llm"
    code_execution_tool: "e2b_code_execution"  # Switch to E2B
    output_folder: "output_data"
    max_retries: 1
```

**Installing E2B SDK:**

```bash
# Install E2B Code Interpreter SDK v2.x
uv pip install "e2b-code-interpreter>=0.2.0"

# Or install ALM example with E2B support
uv pip install -e "examples/asset_lifecycle_management[e2b]"

# Verify installation
python -c "from e2b_code_interpreter import Sandbox; print('✅ E2B SDK installed')"
```

**Troubleshooting E2B:**

```bash
# Error: "E2B SDK not installed"
uv pip install "e2b-code-interpreter>=0.2.0"

# Error: "E2B API key not set" or "SandboxBase.__init__() got an unexpected keyword argument 'api_key'"
# The SDK v2.x REQUIRES environment variable, NOT constructor parameter
export E2B_API_KEY="your-key-from-dashboard"
echo $E2B_API_KEY  # Verify

# Error: "workspace_files_dir not found"
mkdir -p output_data

# Error: "SSL: CERTIFICATE_VERIFY_FAILED" (macOS specific)
# This is a local Python SSL certificate issue, not an E2B API problem
# Fix 1: Install Python certificates (recommended)
/Applications/Python\ 3.11/Install\ Certificates.command

# Fix 2: Or run Python's certificate installer
python -m pip install --upgrade certifi

# Fix 3: Or update macOS root certificates
# Open Keychain Access > System Roots > File > Import Items > Add CA certificates

# Files not downloading - check E2B dashboard for:
# - API quota limits
# - Sandbox execution logs
# - File size limits (100MB max)

# Test E2B connection (requires valid API key)
E2B_API_KEY="your-key" python -c "from e2b_code_interpreter import Sandbox; Sandbox.create()"
```

**SSL Certificate Setup for Remote Machines:**

When deploying to remote servers, ensure SSL certificates are configured:

```bash
# Ubuntu/Debian
sudo apt-get install -y ca-certificates
pip install --upgrade certifi

# Verify SSL
python -c "import ssl; import urllib.request; urllib.request.urlopen('https://api.e2b.dev')"

# Test E2B connection
E2B_API_KEY="your-key" python -c "from e2b_code_interpreter import Sandbox; Sandbox.create()"
```

See `configs/SSL_CERTIFICATE_SETUP.md` for comprehensive guide covering:
- SSL setup for Ubuntu, CentOS, macOS, Docker
- Troubleshooting SSL certificate errors
- Environment-specific configurations
- CI/CD pipeline setup
- Best practices checklist

**Important**: The ALM example uses **local Docker sandbox by default** (no SSL needed). E2B is an optional cloud alternative.

**Documentation References:**
- E2B Test Guide: `examples/asset_lifecycle_management/configs/E2B_TEST_README.md`
- SSL Setup Guide: `examples/asset_lifecycle_management/configs/SSL_CERTIFICATE_SETUP.md`
- E2B Sandbox Code: `examples/asset_lifecycle_management/src/nat_alm_agent/code_execution/e2b_sandbox.py`
- Test Script: `examples/asset_lifecycle_management/test_e2b_sandbox.py`
- Minimal Config: `examples/asset_lifecycle_management/configs/config-e2b-test.yaml`
- E2B Docs: https://e2b.dev/docs

Workspace utilities pattern:
- Pre-built functions in `output_data/utils/` (copied from `utils_template/`)
- Code generation assistant imports utilities instead of generating complex algorithms
- Provides reliability and consistency for transformations (e.g., `apply_piecewise_rul_transformation`)

Requires:
- NASA C-MAPSS turbofan dataset in `data/` directory
- SQLite database (or PostgreSQL/MySQL with optional dependencies)
- Docker for code execution sandbox
- MOMENT library (cloned locally, installed via `[tool.uv.sources]`)

Dataset setup: `python setup_database.py` after downloading NASA dataset from Kaggle

**Kaggle API Setup for NASA Dataset Download:**

```bash
# Set Kaggle API token (required for downloading datasets)
export KAGGLE_API_TOKEN=KGAT_da3623cbde9f15d964d4efa2d5b8d0e6

# Download NASA C-MAPSS dataset
curl -L -o ~/Downloads/nasa-cmaps.zip \
  https://www.kaggle.com/api/v1/datasets/download/behrad3d/nasa-cmaps

# Extract to data/ directory
cd /Users/vikalluru/Documents/NeMo-Agent-Toolkit-Examples/examples/asset_lifecycle_management
mkdir -p data
unzip ~/Downloads/nasa-cmaps.zip -d data/

# Create database
python setup_database.py
```

**Code execution sandbox options:**

**Option A: Local Docker Sandbox (Default)**
```bash
# From NeMo-Agent-Toolkit repo
cd src/nat/tool/code_execution/
./local_sandbox/start_local_sandbox.sh local-sandbox /path/to/output_data/

# Verify it's running
curl -X POST http://localhost:6000/execute \
  -H 'Content-Type: application/json' \
  -d '{"generated_code": "print(\"Hello!\")", "timeout": 10, "language": "python"}'
```

**Option B: E2B Cloud Sandbox (No Docker Required)**
```bash
# 1. Get API key from https://e2b.dev/dashboard
export E2B_API_KEY="your-e2b-api-key"

# 2. Install E2B dependency
uv pip install -e "examples/asset_lifecycle_management[e2b]"

# 3. Test E2B sandbox directly
cd examples/asset_lifecycle_management
python test_e2b_sandbox.py

# 4. Use in NAT workflows by changing config
# Update code_generation_assistant to use e2b_code_execution tool
```

**Quick E2B test:**
```bash
cd examples/asset_lifecycle_management
export E2B_API_KEY="your-key"
export NVIDIA_API_KEY="your-key"

nat run --config_file configs/config-e2b-test.yaml \
  --input "Create a JSON file with test data"
```

## Important Conventions

### Git Commit Signing

All commits must be GPG-signed:

```bash
git commit -S -m "Your message"

# Configure automatic signing
git config --global commit.gpgsign true
```

Always include co-authorship when working with Claude:
```
Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
```

### Security Best Practices

- Use whitelisting for enum-like inputs (see `mcp_rag_demo/src/nat_mcp_rag_demo/register.py:116` for category validation example)
- Environment variables for API keys, never hardcode
- Validate all user inputs in tool functions

### Code Style

- Max line length: 120 characters
- Import organization: ruff with isort rules (first-party: `nat`, `nat_*`)
- All files require SPDX copyright header
- Pre-commit hooks enforce yapf, ruff, and copyright checks

### External Dependencies

- `nat_vanna_tool`: External package from NeMo-Agent-Toolkit repo, referenced via `[tool.uv.sources]` in ALM example
- MOMENT library: Time-series foundation model, cloned locally for ALM example
- `e2b-code-interpreter`: Cloud sandbox SDK for E2B code execution (optional dependency)
- Modify `pyproject.toml` paths when setting up ALM to match your local paths

### Optional Dependencies in ALM Example

```toml
[project.optional-dependencies]
e2b = ["e2b-code-interpreter>=0.2.0"]  # Cloud code execution
postgres = ["psycopg2-binary>=2.9.0"]  # PostgreSQL support
mysql = ["pymysql>=1.0.0"]             # MySQL support
elasticsearch = ["elasticsearch>=8.0.0"] # Elasticsearch vector store
all = [/* all optional dependencies */]
```

Install specific features:
```bash
uv pip install -e "examples/asset_lifecycle_management[e2b,postgres]"
```

### Package-Based Tool Auto-Registration

External NAT packages (like `nat_vanna_tool`) use automatic registration via entry points:

**In external package's `pyproject.toml`:**
```toml
[project.entry-points.'nat.components']
nat_vanna_tool = "nat_vanna_tool.register"
```

**Registration behavior:**
- Tools are auto-registered when the package is installed (via pip/uv)
- No explicit import needed in your `register.py`
- Tools become available by their config `_type` name in YAML configs
- This is how `vanna_sql_tool` is available without being imported in ALM's `register.py`

**Local vs Package tools:**
- **Local tools**: Explicitly imported in `register.py` (e.g., `from .retrievers import generate_sql_query_and_retrieve_tool`)
- **Package tools**: Auto-registered via entry points, available globally once package installed

**Referencing package dependencies:**
```toml
[tool.uv.sources]
nat_vanna_tool = { path = "../../packages/nat_vanna_tool", editable = true }

dependencies = [
  "nat_vanna_tool",  # Package is referenced here
]
```

This pattern enables sharing reusable tools across examples without code duplication.

### Framework Wrappers

Tools can support multiple AI frameworks using `framework_wrappers` parameter:

```python
@register_function(
    config_type=MyToolConfig,
    framework_wrappers=[LLMFrameworkEnum.LANGCHAIN]
)
async def my_tool(config: MyToolConfig, builder: Builder):
    # Get LangChain-wrapped components
    embedder = await builder.get_embedder(
        config.embedder_name,
        wrapper_type=LLMFrameworkEnum.LANGCHAIN
    )
```

Common wrappers: `LANGCHAIN`, `LLAMA_INDEX`, `NATIVE`
